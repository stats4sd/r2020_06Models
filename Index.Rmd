---
title: "Introduction to Statistical Modelling in R - Part 1"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    df_print: default
runtime: shiny_prerendered
description: >
  Modelling in R
---

```{r setup, include=FALSE}
library(dplyr)
library(ggplot2)
library(ggfortify)
library(learnr)
library(broom)
library(knitr)
library(Lock5Data)
library(emmeans)

predict_data<-data.frame(Years=1:6)

tutorial_options(exercise.timelimit = 10)
options(tibble.width = Inf)
FootballBrain<-Lock5Data::FootballBrain
set.seed(555)
FootballBrain<-FootballBrain[sample(1:nrow(FootballBrain)),]
row.names(FootballBrain)<-NULL
linreg_mod<-lm(Hipp~Years,data=FootballBrain)

```


## Introduction 

In this module we will introduce you to the modeling syntax and functions contained within R. We will focus on the `lm` function, and show how this can do much more than just a simple linear regression!

For part 1, we will work through a simple linear regression model, and then in Part 2 we will look at ways we can extend this.

We will mostly be using base R functions in this module. But we will also introduce quite a few useful functions from other packages to make modeling in R a little bit easier. In particular you will see functions from `broom`, `emmeans`, and `ggfortify` so make sure these packages have been installed and loaded if you are working on your own computer.

We are assuming that you have come across the topic of linear models before. Even if you have, there is a nice video from Crash Course Statistics which provides a nice refresher of the methodology.
![](https://www.youtube.com/watch?v=WWqE7YHR4Jc) 


## Data used in this session

The data used for this session is taken from a 2014 study into the impact of playing American Football on brain damage and cognitive function. It has been imported into this session as a data frame called `FootballBrain`.


Data from 75 individuals is available. There are a mixture of football players with known skull or concussion injuries; football players without known skull or concussion injuries; and non-football players who have a similar demographic profile to the football players. Which of these three groups an individual is part of is shown in the `Group` variable. Other data available is shown below.

```{r,echo=FALSE}
data.frame(Variable=colnames(FootballBrain),
           Explanation=c("Group (explained above)","Total hippocampus volume, in microL",
                         "Left hippocampus volume, in microL","	Number of years playing football","Cognitive testing composite reaction time score, given as a percentile. This is not available for all participants")) %>% kable()
```

In part 1 we will be building a model to try to understand the relationship between total hippocampus volume with the number of years playing football.

You can explore this data a little below:


```{r,echo=FALSE}
DT::datatable(FootballBrain)
```

Reference for the data:

Singh R, Meier T, Kuplicki R, Savitz J, et al., "Relationship of Collegiate Football Experience and Concussion With Hippocampal Volume and Cognitive Outcome," JAMA, 311(18), 2014

## Simple linear regression

### Exploring the Data

Let's first of all investigate our data, to take a look at the relationship between hippocampus volume and number of years playing football. Both of these are numeric variables so we can use a point geometry to make a scatter plot.


```{r ggplot1,exercise=TRUE}
ggplot(data=FootballBrain,aes(y=Hipp,x=Years))+
  geom_point()
```


Looking at this graph, we can see a pretty clear trend - as number of years is increasing the hippocampus volume is decreasing. Because of the way the data is collected we should not be surprised to see the jump from individuals with 0 years experience (the non-footballers) to those with 6 years experience or more (the football players).

A way of summarising this trend, with numbers, could be to break the explanatory variable into groups and then calculate mean values within those new groups. How to split into groups is a little arbitrary, but we could select 0-4 years, 5-9 years, 10-14 years, 15+ years. To do this we could use the `cut()` function within `mutate()` to create a new variable.

```{r cut1, exercise=TRUE}
FootballBrain %>%
  mutate(Year_grp=cut(Years,breaks=c(-0.5,4.5,9.5,14.5,99),
                      labels=c("0-4 years", "5-9 years", "10-14 years", "15+ years")))
```

Within the `cut()` function we need to provide `breaks` denoting which values make up the boundaries for the new groups we are creating and `labels` providing the names for the new groups. There should always be one less `label` than there is a `break`. In this example we have provided five break points, which will create four groups so four labels are needed.

It can sometimes be tricky to work out the logic of how the groups split when dealing with integer data like we have here. For example with breaks of `c(0,10,20)` - would the value of 10 be in the first or second group? And would the value of 0 be used at all? You can find this out from the help menu; but a shortcut I often use to save myself worrying about this is to use decimal values between the integers. This way you can probably be more confident you are defining the groups exactly as you wanted! 

Once we have this new variable we can use `group_by()` and `summarise()` as we have seen before.

```{r cut2,exercise=TRUE}
FootballBrain %>%
  mutate(Year_grp=cut(Years,breaks=c(-0.5,4.5,9.5,14.5,99),
                      labels=c("0-4 years", "5-9 years", "10-14 years", "15+ years"))) %>%
    group_by(Year_grp) %>%
      summarise(mean(Hipp))


```

This now gives a clear numeric summary of how the mean hippocampus volume is decreasing with the increase in number of years of playing football.

### Correlation

A simple way of summarising this relationship, that you probably learnt in school, could be through calculating the correlation coefficient. This has a function in R called `cor()`. We can use this through the `summarise()` function similarly to what we have seen before, but `cor()` requires the names of two columns, both of which must be numeric. 


```{r cor0,exercise=TRUE}
FootballBrain %>%
  summarise(cor(Hipp,Years))
```

The correlation is pretty strong, -0.69, as we might expect from looking at the plot. It is negative, because hippocampus volume is decreasing with an increase in years of football playing. 
But correlation coefficient is a very limited type of analysis - we can learn much more through using a modelling approach.

### Fitting a line to the plot

You probably learnt at school about the idea of 'putting a line through' your points when you see a scatter plot like this. We have already learnt about `geom_smooth`, but as discussed previously it defaults to a moving average type model. However we can change the `method` argument to fit a simple straight line instead. Looking at the plot, a straight line does look like a reasonable choice for modelling this data.

```{r plotlm,exercise=TRUE}
ggplot(data=FootballBrain,aes(y=Hipp,x=Years))+
  geom_point()+
    geom_smooth(method="lm")
```
By default we see a 95% confidence interval shaded around the trend line.

### Fitting the Model

To examine what we actually have in our model, we need to fit the model ourself. We do this using the `lm()` command. The syntax for this is very similar to what we saw in the previous module for t-tests, with a response variable, a tilde, then the explanatory variable. 
Like with the `t.test` function, the data argument comes after the formula. So if we were to use a pipe from data into `lm()` we would need to specify `data=.` within the `lm` function.

```{r mdo1,exercise=TRUE}
lm(Hipp~Years,data=FootballBrain)
```

The output here only tells us two things: 

* "Call" - simply repeating back the model we have specified 
* "Coefficients": Telling us the values of the parameters
 
A linear regression follows the equation of a straight line y = B0 + B1x. You may have learnt this same equation as y=a+bx or y=mx+c ; depending on where and when you went to school.

The coefficients give us the value of our intercept (B0): 7599 and the value of our slope (B1): -130.

So the overall model would be:

Hippocampus Volume (microL)  = 7599  - 130 * Years playing football + $\epsilon$

The value of the intercept is interpreted as the expected value of the response variable where all explanatory variables are equal to zero. So for someone with no years of football playing, we would expect to see an average hippocampus volume of 7599 microL.

The value of the slope represents the change we expect to see in the response variable for a one unit increase in the explanatory variable. So for every year of increased football playing, we expect an average reduction in the average hippocampus volume of 130 microL.

The output at this stage has not told us anything about the error term, $\epsilon$. 

However we can get much more information from our model than this! R is a little different from many other software packages, which will produce a lot of different outputs when you create a model. In R you have to ask specifically for what you want out of a model.

But first we usually need to save the model to an object. I am choosing to give it the name `linreg_mod`.

```{r mod_obj,exercise=TRUE}
linreg_mod<-lm(Hipp~Years,data=FootballBrain)

```

As ever, when creating an object we don't see any immediate output. But we now have the ability to use lots of functions that give us different pieces of output and inference from this model. 

### Summarising the model

```{r summary,exercise=TRUE}
summary(linreg_mod)
```

`summary()` provides us with a lot of useful information in a customised output format containing model fit statistics, standard errors and p-values for the coefficients.

The `broom` library also provides the functions `glance()` and `tidy()`. This extracts very similar information to `summary()` but returns it in the form of data frames. This can be very useful if we want to export results from this model into other analyses or to make the output look nice in our reports. 

```{r glance0,exercise=TRUE}
glance(linreg_mod)
```

```{r tidy,exercise=TRUE}
tidy(linreg_mod)
```
In general - `summary()` is useful if all you want to do is look at the model output; the output is designed to be easier for humans to read. `glance()` and `tidy()` are better if you want to use that model output in some way; the output is designed to be easier for computers to read.

One of the outputs from `summary()` or `glance()` allows us to complete the full linear regression model, as it provides the value of sigma, also known as the residual standard error. Within our error term, $\epsilon$, this is the standard deviation of the residual values. Because we are doing a simple linear model we assume our residuals are normally distributed with a mean of 0. We will check if that assumption makes sense in a short while.

Overall the information tells us that the relationship between the Years playing football and hippocampus volume is highly significant. p=1.05e-11. i.e. p=0.00000000001.

The intercept is also significant. But this is almost certainly not of interest. The null hypothesis which generates this p-value is that the intercept is equal to 0. In other words the null hypothesis is that someone who had never played football would literally have no brain, or at least a brain with 0 hippocampus volume. It is not really surprising to see very strong evidence against this nonsensical null hypothesis! 

If we look back at the output of `summary()` or `glance()`, we can also see that 47% of the variability in hippocampus volume can be explained by the linear increase in number of years playing football (Multiple R- Squared / r.squared). You can read a little bit about r-squared, and why the adjusted r-squared is also a useful metric to consider [here](https://thestatsgeek.com/2013/10/28/r-squared-and-adjusted-r-squared/).

We have actually seen this 47% number before (sort of). Look at the square root of this number:

```{r sqrt1,exercise=TRUE}
sqrt(0.4713346)
```

*Where have you seen this number before?*

Maybe you remember it from here:
```{r cor2,exercise=TRUE}
cor(FootballBrain$Hipp,FootballBrain$Years)
```
The R Squared value from a simple linear model is equal to the correlation coefficient squared. There are also other model fit statistics in the output from `glance()` (AIC, BIC, Deviance) which we will talk more about in Part 2. 

The function `confint()` will give 95% confidence intervals around the parameters:

```{r ci1,exercise=TRUE}
confint(linreg_mod)
```

So that we can see that our 95% confidence interval around the average decrease in the hippocampus volume by year of football goes from -162 to -97. As before, we should probably ignore the numbers for the intercept in this model

### Checking Model

We should also check our residual plots to assess model validity. It is worth recapping, or learning how to interpret these plots, as it can take some practice to know what you are looking for. "Statistics by Jim" has a nice overview: https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/

```autoplot(model)``` from the `ggfortify` package produces the four standard model checking plots. 

```{r check_res,exercise=TRUE,message=FALSE,warning=FALSE}
autoplot(linreg_mod)
```

Do these plots look OK? What are we actually looking for here?

In this case, most of the plots look fine: 

* There are no clear trends in the first plot. Suggesting that a straight line is a sensible model.
* The second plot shows points lining up nicely, suggested no major deviation from normality. 
* In the third plot there is some evidence of an increase in variability with increasing predicted value. But this is not a major deviation - the trend line being drawn on the scale location plot perhaps looks worse than it should, because of how well predicted the two lowest values in the data were.
* In the fourth plot there are no points that we would consider to have high leverage, and only two points with moderately large standardised residuals. These would not be enough to case us concern.

If we did see problems, there are several things we could do to try to improve our model. Our actions would depend on what problems we observed.

* Add extra variables into the model. Other variables may help to explain why there are non-linear patterns, or why some values appear to be outliers. 

* Remove outliers, and conduct a sensitivity analysis. By comparing results with and without individual strange looking points, this will help us understand whether these outliers have any impact on our conclusions.

* Transform variables. Different transformations may help to model non-linear relationships, particular considering different types of curves. Log transformations can also help with heterogeneity, in particular if variance is increasing as the response variable increases. Sometimes transformations might help to deal with issues with normality as well.

* With severe violations of normality, or heterogeneity, we may consider moving to a generalised linear model. This will make a different distributional assumption about our residuals.

There is also a base-R function to make these plots, simply `plot`, which you may see used a lot if you follow other tutorials. But `autoplot` produces much nicer looking, `ggplot2` compatible output plots.

### Making predictions

Remember in our original data we had nobody who had between 1 and 6 years of football experience. If we are happy we have a sensible model then we can make predictions about what we would expect the average hippocampus volume to be for footballers with that level of experience.

To make predictions we first need to create a new data frame containing the values we want to predict. In this example we just have one explanatory variable, so we create a data frame with just one variable. The variable name in the new data frame has to match the variable name from the original data. If we had a model with more than one variable we would need to make sure that the prediction data frame contains new variables for all of the variables from the model.

```{r prd_data,exercise=TRUE}
predict_data<-data.frame(Years=1:6)
predict_data
```


We can either use the function `predict()` or `augment()`.

`predict()` is the function from base-R, which will return just the predicted values. 
I prefer the `augment()` function from the `broom` library which adds the predictions into the existing data frame. You can see the difference in the output below, but the syntax within these two functions is identical: the name of the model, followed by a `newdata=` argument for the name of the data frame we want to predict over.

```{r aug_pred,exercise=TRUE}

augment(linreg_mod,newdata=predict_data)

predict(linreg_mod,newdata=predict_data)



```

You can see the fitted values are the same from both functions, but the output from `augment` returns the predictions in a more usable format, and also provides standard errors. 

We might want to add these predictions onto the plot we made earlier. 

```{r pred_plot,exercise=TRUE}
predict_data<-augment(linreg_mod,newdata=predict_data)

ggplot(data=FootballBrain,aes(y=Hipp,x=Years))+
  geom_point()+
      geom_point(data=predict_data,aes(y=.fitted,x=Years),inherit.aes = FALSE,color="red",size=4)
      
```

This particular plot is maybe not that useful, since drawing the line with `geom_smooth` is a better way of showing the same information. 

But I think it is useful to know that in ggplot, you can set different data sets to be used within each `geom`. This is something we have not seen before!
You are able to specify different data and/or aesthetics to be used within any geometry, as long as you also include the argument `inherit.aes=FALSE`.



## References 

Simple Linear Regression in R (Marin Stats Lectures): https://www.youtube.com/watch?v=66z_MRwtFJM
There are lots of subsequent videos on this channel about other statistical modelling techniques

An R Companion to Applied Regression
https://socialsciences.mcmaster.ca/jfox/Books/Companion/downloads.html

Linear Regression Tutorial: (UC Business Analytics R Programming Guide) https://uc-r.github.io/linear_regression

Common statistical tests are linear models (Jonas Lindeløv):
https://lindeloev.github.io/tests-as-linear/

When to use regression Analysis (Statistics by Jim): https://statisticsbyjim.com/regression/when-use-regression-analysis/

UCLA Data Analysis Examples: https://stats.idre.ucla.edu/other/dae/
